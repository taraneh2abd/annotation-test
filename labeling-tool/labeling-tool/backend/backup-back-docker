FROM python:3.11-slim

WORKDIR /app
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

# System deps
# --- System dependencies (for PyTorch + Pillow/OpenCLIP) ---
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libglib2.0-0 \
    libsm6 \
    libxrender1 \
    libxext6 \
    git \
 && rm -rf /var/lib/apt/lists/*

# Python deps
COPY requirements.txt .
RUN pip install  -r requirements.txt

# --- Install Torch + CUDA-enabled backend ---
RUN pip install  --trusted-host download.pytorch.org \
    torch==2.4.1 --index-url https://download.pytorch.org/whl/cu121

# --- Install OpenCLIP and dependencies (no optional bloat) ---
RUN pip install  open-clip-torch==2.24.0 --no-deps && \
    pip install  \
        numpy==1.26.4 \
        scikit-learn==1.5.2 \
        pillow==10.4.0 \
        tqdm==4.66.5 \
        ftfy==6.3.1 \
        regex==2025.9.18 \
        sentencepiece==0.2.1 \
        timm==1.0.20 \
        torchvision==0.23.0 \
        protobuf==6.32.1 \
        huggingface-hub==0.35.3



# --- Cache OpenCLIP ViT-B-16 (laion2b_s34b_b88k) model weights ---
RUN python - <<'PYCODE'
import os, sys, requests
from huggingface_hub import HfApi, model_info
from tqdm import tqdm

model_id = "laion/CLIP-ViT-B-16-laion2B-s34B-b88K"  # ✅ correct public model ID
cache_dir = os.path.expanduser("~/.cache/clip")
print(f"[cache] Ensuring {model_id} is cached in {cache_dir}")

api = HfApi()
info = model_info(model_id)
files = [f.rfilename for f in info.siblings if f.rfilename.endswith((".bin", ".json"))]

def download_with_progress(url, dest):
    r = requests.get(url, stream=True)
    r.raise_for_status()
    total = int(r.headers.get("content-length", 0))
    with open(dest, 'wb') as f, tqdm(total=total, unit='B', unit_scale=True,
                                     desc=os.path.basename(dest), file=sys.stdout) as bar:
        for chunk in r.iter_content(1024 * 1024):
            f.write(chunk)
            bar.update(len(chunk))

for filename in files:
    dest_path = os.path.join(cache_dir, filename)
    os.makedirs(os.path.dirname(dest_path), exist_ok=True)
    if os.path.exists(dest_path):
        print(f"[cache] ✅ {filename} already cached.")
        continue
    url = f"https://huggingface.co/{model_id}/resolve/main/{filename}"
    print(f"[cache] downloading {filename} ...")
    try:
        download_with_progress(url, dest_path)
        print(f"[cache] ✅ {filename} cached at {dest_path}")
    except Exception as e:
        print(f"[cache] ⚠️ Failed to download {filename}: {e}")

# Verify load
import open_clip
print("[cache] verifying model load ...")
open_clip.create_model_and_transforms('ViT-B-16', pretrained='laion2b_s34b_b88k')
print("[cache] ✅ Model cached and verified successfully.")
PYCODE



# App code
COPY main.py .

EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]